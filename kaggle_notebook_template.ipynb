{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31ae55c",
   "metadata": {},
   "source": [
    "# Model Merging with SparseGPT on Kaggle\n",
    "\n",
    "This notebook demonstrates how to merge fine-tuned LLaMA models using three methods:\n",
    "1. TIES (magnitude-based)\n",
    "2. DARE (random dropout)\n",
    "3. SparseGPT (Hessian-based importance)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Enable GPU (Settings ‚Üí Accelerator ‚Üí GPU T4 x2)\n",
    "- Enable Internet (Settings ‚Üí Internet ‚Üí On)\n",
    "- Add your fine-tuned models as Kaggle datasets\n",
    "- ‚ö†Ô∏è **NO HuggingFace account needed!** (unless using private models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52793f79",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup script\n",
    "!python kaggle_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy models from Kaggle input to working directory\n",
    "# Update the paths below to match your Kaggle dataset names\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Example: Copy your fine-tuned model(s)\n",
    "kaggle_datasets = [\n",
    "    \"/kaggle/input/llama3-2-1b-instruct-ft-doctor-consulting\",\n",
    "    # Add more dataset paths here if you have multiple models\n",
    "]\n",
    "\n",
    "local_models = []\n",
    "\n",
    "for i, dataset_path in enumerate(kaggle_datasets):\n",
    "    if os.path.exists(dataset_path):\n",
    "        local_path = f\"./finetuned_model_{i+1}\"\n",
    "        print(f\"Copying {dataset_path} ‚Üí {local_path}\")\n",
    "        shutil.copytree(dataset_path, local_path)\n",
    "        \n",
    "        # Verify files\n",
    "        files = os.listdir(local_path)\n",
    "        print(f\"  ‚úì Copied {len(files)} files\")\n",
    "        \n",
    "        local_models.append(local_path)\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Dataset not found: {dataset_path}\")\n",
    "\n",
    "print(f\"\\n‚úì Ready to use {len(local_models)} model(s)\")\n",
    "print(f\"Local paths: {local_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31291bdd",
   "metadata": {},
   "source": [
    "## Step 1A: Copy Models from Kaggle Datasets (If Using Kaggle Datasets)\n",
    "\n",
    "**IMPORTANT:** If your fine-tuned models are uploaded as Kaggle datasets, copy them first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d268a",
   "metadata": {},
   "source": [
    "## Step 2: Configure HuggingFace Token (OPTIONAL - Only if using HF models)\n",
    "\n",
    "**Skip this step if you copied models from Kaggle datasets above!**\n",
    "\n",
    "Only needed if:\n",
    "- Using models from HuggingFace Hub\n",
    "- Using private HuggingFace models\n",
    "- Base model requires authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Use Kaggle Secrets (recommended)\n",
    "# Add HF_TOKEN in Kaggle Secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# Option 2: Direct token (not recommended for public notebooks)\n",
    "# hf_token = \"hf_...\"\n",
    "\n",
    "login(token=hf_token)\n",
    "print(\"‚úì Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caabe0c",
   "metadata": {},
   "source": [
    "## Step 3: Configure Merging Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_merge import LLaMAMerger\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# OPTION A: Using models copied from Kaggle datasets (from Step 1A)\n",
    "# Use the local_models list created above\n",
    "FINETUNED_MODELS = local_models  # [\"./finetuned_model_1\", \"./finetuned_model_2\", ...]\n",
    "\n",
    "# OPTION B: Using HuggingFace model IDs (requires Step 2)\n",
    "# FINETUNED_MODELS = [\n",
    "#     \"your-username/model-name\",\n",
    "#     \"your-username/another-model\",\n",
    "# ]\n",
    "\n",
    "# Calibration datasets (one per model)\n",
    "# These can be HuggingFace dataset names\n",
    "DATASETS = [\n",
    "    \"Amod/mental_health_counseling_conversations\",\n",
    "    # Add more datasets here (one per fine-tuned model)\n",
    "]\n",
    "\n",
    "# Make sure you have same number of datasets as models\n",
    "if len(DATASETS) < len(FINETUNED_MODELS):\n",
    "    # Duplicate the first dataset if needed\n",
    "    DATASETS = DATASETS * len(FINETUNED_MODELS)\n",
    "    DATASETS = DATASETS[:len(FINETUNED_MODELS)]\n",
    "\n",
    "# Parameters\n",
    "OUTPUT_DIR = \"./merged_models\"\n",
    "CACHE_DIR = \"./merge_cache\"\n",
    "DENSITY = 0.2  # Keep top 20% of weights\n",
    "NUM_CALIBRATION_SAMPLES = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Fine-tuned models: {FINETUNED_MODELS}\")\n",
    "print(f\"Datasets: {DATASETS}\")\n",
    "print(f\"Density: {DENSITY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499b6c4",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd80097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create merger instance\n",
    "merger = LLaMAMerger(\n",
    "    base_model_path=BASE_MODEL,\n",
    "    finetuned_model_paths=FINETUNED_MODELS,\n",
    "    dataset_names=DATASETS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    density=DENSITY,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"‚úì Merger initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e4bdd",
   "metadata": {},
   "source": [
    "## Step 5: Run All Methods and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will:\n",
    "# 1. Compute task vectors (if not cached)\n",
    "# 2. Compute Hessians (if not cached)\n",
    "# 3. Merge with all three methods\n",
    "# 4. Evaluate and compare\n",
    "\n",
    "results = merger.merge_all_methods()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Perplexity: {metrics['perplexity']:.4f}\")\n",
    "    print(f\"  Time: {metrics['time']:.2f}s\")\n",
    "\n",
    "# Find best method\n",
    "best_method = min(results, key=lambda k: results[k]['perplexity'])\n",
    "print(f\"\\nüèÜ Best method: {best_method}\")\n",
    "print(f\"   Perplexity: {results[best_method]['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab178e",
   "metadata": {},
   "source": [
    "## Step 6: Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models are already saved in OUTPUT_DIR\n",
    "# You can load and test them:\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the best model (usually TIES-SparseGPT)\n",
    "best_model_path = f\"{OUTPUT_DIR}/ties_sparsegpt_merged\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(best_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "print(f\"‚úì Loaded best model from {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5202e",
   "metadata": {},
   "source": [
    "## Step 7: Test the Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample prompt\n",
    "prompt = \"I've been feeling anxious lately. What should I do?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc83c2d",
   "metadata": {},
   "source": [
    "## Optional: Run Individual Methods\n",
    "\n",
    "If you want to run just one method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only SparseGPT method\n",
    "sparsegpt_model = merger.merge_with_ties(use_sparsegpt=True)\n",
    "sparsegpt_model.save_pretrained(f\"{OUTPUT_DIR}/sparsegpt_only\")\n",
    "\n",
    "print(\"‚úì SparseGPT model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3688e",
   "metadata": {},
   "source": [
    "## Optional: Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43719e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the best model to HuggingFace\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_name = \"your-username/merged-mental-health-counselor\"\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"‚úì Model uploaded to {repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8aee6",
   "metadata": {},
   "source": [
    "## Memory Monitoring (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4875f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Clear cache if needed\n",
    "    # torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

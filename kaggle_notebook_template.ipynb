{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52793f79",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup script\n",
    "!python kaggle_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d268a",
   "metadata": {},
   "source": [
    "## Step 2: Configure HuggingFace Token (Optional)\n",
    "\n",
    "If using private models or need authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Use Kaggle Secrets (recommended)\n",
    "# Add HF_TOKEN in Kaggle Secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# Option 2: Direct token (not recommended for public notebooks)\n",
    "# hf_token = \"hf_...\"\n",
    "\n",
    "login(token=hf_token)\n",
    "print(\"‚úì Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caabe0c",
   "metadata": {},
   "source": [
    "## Step 3: Configure Merging Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_merge import LLaMAMerger\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Your fine-tuned models (local paths or HF IDs)\n",
    "FINETUNED_MODELS = [\n",
    "    \"llama-3.2-1b-mental-health-counselor\",\n",
    "    # Add more models here\n",
    "]\n",
    "\n",
    "# Calibration datasets (one per model)\n",
    "DATASETS = [\n",
    "    \"Amod/mental_health_counseling_conversations\",\n",
    "    # Add corresponding datasets\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "OUTPUT_DIR = \"./merged_models\"\n",
    "CACHE_DIR = \"./merge_cache\"\n",
    "DENSITY = 0.2  # Keep top 20% of weights\n",
    "NUM_CALIBRATION_SAMPLES = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"Fine-tuned models: {len(FINETUNED_MODELS)}\")\n",
    "print(f\"Density: {DENSITY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499b6c4",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd80097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create merger instance\n",
    "merger = LLaMAMerger(\n",
    "    base_model_path=BASE_MODEL,\n",
    "    finetuned_model_paths=FINETUNED_MODELS,\n",
    "    dataset_names=DATASETS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    density=DENSITY,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"‚úì Merger initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e4bdd",
   "metadata": {},
   "source": [
    "## Step 5: Run All Methods and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will:\n",
    "# 1. Compute task vectors (if not cached)\n",
    "# 2. Compute Hessians (if not cached)\n",
    "# 3. Merge with all three methods\n",
    "# 4. Evaluate and compare\n",
    "\n",
    "results = merger.merge_all_methods()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Perplexity: {metrics['perplexity']:.4f}\")\n",
    "    print(f\"  Time: {metrics['time']:.2f}s\")\n",
    "\n",
    "# Find best method\n",
    "best_method = min(results, key=lambda k: results[k]['perplexity'])\n",
    "print(f\"\\nüèÜ Best method: {best_method}\")\n",
    "print(f\"   Perplexity: {results[best_method]['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab178e",
   "metadata": {},
   "source": [
    "## Step 6: Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models are already saved in OUTPUT_DIR\n",
    "# You can load and test them:\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the best model (usually TIES-SparseGPT)\n",
    "best_model_path = f\"{OUTPUT_DIR}/ties_sparsegpt_merged\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(best_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "print(f\"‚úì Loaded best model from {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5202e",
   "metadata": {},
   "source": [
    "## Step 7: Test the Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample prompt\n",
    "prompt = \"I've been feeling anxious lately. What should I do?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc83c2d",
   "metadata": {},
   "source": [
    "## Optional: Run Individual Methods\n",
    "\n",
    "If you want to run just one method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only SparseGPT method\n",
    "sparsegpt_model = merger.merge_with_ties(use_sparsegpt=True)\n",
    "sparsegpt_model.save_pretrained(f\"{OUTPUT_DIR}/sparsegpt_only\")\n",
    "\n",
    "print(\"‚úì SparseGPT model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3688e",
   "metadata": {},
   "source": [
    "## Optional: Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43719e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the best model to HuggingFace\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_name = \"your-username/merged-mental-health-counselor\"\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"‚úì Model uploaded to {repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8aee6",
   "metadata": {},
   "source": [
    "## Memory Monitoring (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4875f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Clear cache if needed\n",
    "    # torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

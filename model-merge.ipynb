{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31ae55c",
   "metadata": {},
   "source": [
    "# Model Merging with SparseGPT on Kaggle\n",
    "\n",
    "This notebook demonstrates how to merge fine-tuned LLaMA models using three methods:\n",
    "1. TIES (magnitude-based)\n",
    "2. DARE (random dropout)\n",
    "3. SparseGPT (Hessian-based importance)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Enable GPU (Settings ‚Üí Accelerator ‚Üí GPU T4 x2)\n",
    "- Enable Internet (Settings ‚Üí Internet ‚Üí On)\n",
    "- Add your fine-tuned models as Kaggle datasets\n",
    "- ‚ö†Ô∏è **NO HuggingFace account needed!** (unless using private models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52793f79",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c2500b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T06:58:14.202512Z",
     "iopub.status.busy": "2025-11-17T06:58:14.201655Z",
     "iopub.status.idle": "2025-11-17T06:59:52.367820Z",
     "shell.execute_reply": "2025-11-17T06:59:52.367061Z",
     "shell.execute_reply.started": "2025-11-17T06:58:14.202484Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'test-2'...\n",
      "remote: Enumerating objects: 24, done.\u001b[K\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 24 (delta 4), reused 23 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (24/24), 62.26 KiB | 4.45 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "/kaggle/working/test-2\n",
      "================================================================================\n",
      "KAGGLE ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "\n",
      "[1/6] Checking GPU availability...\n",
      "‚úì GPU available: Tesla P100-PCIE-16GB\n",
      "  Memory: 17.06 GB\n",
      "\n",
      "[2/6] Installing required packages...\n",
      "  Installing transformers>=4.36.0...\n",
      "  Installing datasets>=2.14.0...\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  Installing peft>=0.7.0...\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  Installing accelerate>=0.25.0...\n",
      "  Installing sentencepiece...\n",
      "  Installing protobuf...\n",
      "‚úì All packages installed\n",
      "\n",
      "[3/6] Repository setup...\n",
      "  Cloning repository from https://github.com/Shahriar-Ferdoush/test-2.git...\n",
      "Cloning into 'Model-Merging'...\n",
      "remote: Enumerating objects: 24, done.\u001b[K\n",
      "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 24 (delta 4), reused 23 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (24/24), 62.26 KiB | 3.66 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n",
      "‚úì Repository cloned\n",
      "\n",
      "[4/6] Setting up paths...\n",
      "‚úì Working directory: /kaggle/working/test-2/Model-Merging\n",
      "\n",
      "[5/6] Creating directories...\n",
      "  Created: ./merge_cache\n",
      "  Created: ./merge_cache/task_vectors\n",
      "  Created: ./merge_cache/hessians\n",
      "  Created: ./merged_models\n",
      "‚úì Directories ready\n",
      "\n",
      "[6/6] Checking disk space...\n",
      "‚úì Available disk space: 19.37 GB\n",
      "‚ö† Warning: Less than 20GB free. Model merging may fail.\n",
      "\n",
      "================================================================================\n",
      "SETUP COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Run: from example_mental_health_merge import main\n",
      "2. Or customize parameters and run manually\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run setup script\n",
    "!git clone https://github.com/Shahriar-Ferdoush/test-2.git\n",
    "%cd test-2\n",
    "!python kaggle_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533a6d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T06:59:52.369705Z",
     "iopub.status.busy": "2025-11-17T06:59:52.369473Z",
     "iopub.status.idle": "2025-11-17T06:59:52.582560Z",
     "shell.execute_reply": "2025-11-17T06:59:52.582009Z",
     "shell.execute_reply.started": "2025-11-17T06:59:52.369681Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying /kaggle/input/llama3-2-1b-instruct-ft-doctor-consulting ‚Üí ./finetuned_model_1\n",
      "  ‚úì Copied 6 files\n",
      "Copying /kaggle/input/llama3-2-1b-instruct-fine-tuning-therapist-ai ‚Üí ./finetuned_model_2\n",
      "  ‚úì Copied 6 files\n",
      "\n",
      "‚úì Ready to use 2 model(s)\n",
      "Local paths: ['./finetuned_model_1', './finetuned_model_2']\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA adapter paths - NO COPYING NEEDED!\n",
    "# Kaggle /kaggle/input/ is READ-ONLY but we can LOAD from it directly\n",
    "import os\n",
    "\n",
    "# Your Kaggle dataset paths - these point directly to the LoRA adapters\n",
    "# Adjust the subdirectory name if different\n",
    "LORA_ADAPTERS = [\n",
    "    \"/kaggle/input/llama3-2-1b-instruct-ft-doctor-consulting/llama-3-1b-medical-chatbot-v1\",\n",
    "    \"/kaggle/input/llama3-2-1b-instruct-fine-tuning-therapist-ai/llama-3-1b-therapist-ai-v1\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LORA ADAPTER CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "valid_adapters = []\n",
    "\n",
    "for i, adapter_path in enumerate(LORA_ADAPTERS, 1):\n",
    "    print(f\"\\n[{i}] Checking: {adapter_path}\")\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        files = os.listdir(adapter_path)\n",
    "        \n",
    "        # Check for required LoRA files\n",
    "        has_adapter_config = \"adapter_config.json\" in files\n",
    "        has_adapter_model = any(\"adapter_model\" in f for f in files)\n",
    "        \n",
    "        print(f\"    Files found: {len(files)}\")\n",
    "        print(f\"    adapter_config.json: {'‚úÖ' if has_adapter_config else '‚ùå'}\")\n",
    "        print(f\"    adapter_model.*: {'‚úÖ' if has_adapter_model else '‚ùå'}\")\n",
    "        \n",
    "        if has_adapter_config and has_adapter_model:\n",
    "            print(f\"    ‚úÖ VALID LoRA adapter\")\n",
    "            valid_adapters.append(adapter_path)\n",
    "        else:\n",
    "            print(f\"    ‚ùå INVALID - missing required files\")\n",
    "            print(f\"       Available files: {files[:10]}\")\n",
    "    else:\n",
    "        print(f\"    ‚ùå PATH NOT FOUND\")\n",
    "        print(f\"       Make sure dataset is added to notebook inputs\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RESULT: Found {len(valid_adapters)}/{len(LORA_ADAPTERS)} valid adapter(s)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "if len(valid_adapters) == 0:\n",
    "    raise ValueError(\"No valid LoRA adapters found! Check paths and dataset names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31291bdd",
   "metadata": {},
   "source": [
    "## Step 2: Configure LoRA Adapter Paths\n",
    "\n",
    "**IMPORTANT:** \n",
    "- `/kaggle/input/` is READ-ONLY but we can load models directly from it\n",
    "- NO NEED to copy! Just use the paths directly\n",
    "- Make sure you've added your fine-tuned model datasets to notebook inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d268a",
   "metadata": {},
   "source": [
    "## Step 2: Configure HuggingFace Token (OPTIONAL - Only if using HF models)\n",
    "\n",
    "**Skip this step if you copied models from Kaggle datasets above!**\n",
    "\n",
    "Only needed if:\n",
    "- Using models from HuggingFace Hub\n",
    "- Using private HuggingFace models\n",
    "- Base model requires authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e449a5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T06:59:52.583549Z",
     "iopub.status.busy": "2025-11-17T06:59:52.583316Z",
     "iopub.status.idle": "2025-11-17T06:59:52.587270Z",
     "shell.execute_reply": "2025-11-17T06:59:52.586550Z",
     "shell.execute_reply.started": "2025-11-17T06:59:52.583525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# # Option 1: Use Kaggle Secrets (recommended)\n",
    "# # Add HF_TOKEN in Kaggle Secrets\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# # Option 2: Direct token (not recommended for public notebooks)\n",
    "# # hf_token = \"hf_...\"\n",
    "\n",
    "# login(token=hf_token)\n",
    "# print(\"‚úì Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caabe0c",
   "metadata": {},
   "source": [
    "## Step 3: Configure Merging Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d32e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T06:59:52.589062Z",
     "iopub.status.busy": "2025-11-17T06:59:52.588871Z",
     "iopub.status.idle": "2025-11-17T07:00:25.548467Z",
     "shell.execute_reply": "2025-11-17T07:00:25.547766Z",
     "shell.execute_reply.started": "2025-11-17T06:59:52.589046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 07:00:05.832291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763362806.034506      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763362806.091118      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Base model: /kaggle/input/llama-3.2/transformers/1b-instruct/1\n",
      "Fine-tuned models: ['./finetuned_model_1', './finetuned_model_2']\n",
      "Datasets: ['Amod/mental_health_counseling_conversations', 'ruslanmv/ai-medical-chatbot']\n",
      "Density: 0.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_merge import LLaMAMerger\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Using direct paths from Kaggle input\n",
    "# ============================================================================\n",
    "\n",
    "# Base model (Kaggle dataset)\n",
    "BASE_MODEL = \"/kaggle/input/llama-3.2/transformers/1b-instruct/1\"\n",
    "\n",
    "# LoRA adapters (from previous cell) - NO COPYING, direct paths!\n",
    "FINETUNED_MODELS = valid_adapters\n",
    "\n",
    "# Calibration datasets (HuggingFace dataset IDs - will be downloaded)\n",
    "DATASETS = [\n",
    "    \"ruslanmv/ai-medical-chatbot\",  # Matches medical chatbot adapter\n",
    "    \"Amod/mental_health_counseling_conversations\",  # Matches therapist adapter\n",
    "]\n",
    "\n",
    "# Merging parameters\n",
    "OUTPUT_DIR = \"./merged_models\"\n",
    "CACHE_DIR = \"./merge_cache\"\n",
    "DENSITY = 0.2  # Keep top 20% of weights\n",
    "NUM_CALIBRATION_SAMPLES = 64\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*80)\n",
    "print(\"MERGER CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"\\nBase Model:\")\n",
    "print(f\"  {BASE_MODEL}\")\n",
    "print(f\"\\nLoRA Adapters ({len(FINETUNED_MODELS)}):\")\n",
    "for i, model in enumerate(FINETUNED_MODELS, 1):\n",
    "    print(f\"  {i}. {model}\")\n",
    "print(f\"\\nCalibration Datasets ({len(DATASETS)}):\")\n",
    "for i, dataset in enumerate(DATASETS, 1):\n",
    "    print(f\"  {i}. {dataset}\")\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Density: {DENSITY} (keep top {int(DENSITY*100)}% of weights)\")\n",
    "print(f\"  Calibration samples: {NUM_CALIBRATION_SAMPLES}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Cache directory: {CACHE_DIR}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize merger\n",
    "print(\"\\nInitializing LLaMAMerger...\")\n",
    "merger = LLaMAMerger(\n",
    "    base_model_path=BASE_MODEL,\n",
    "    finetuned_model_paths=FINETUNED_MODELS,\n",
    "    dataset_names=DATASETS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    density=DENSITY,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    device=DEVICE\n",
    ")\n",
    "print(\"‚úÖ Merger initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499b6c4",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Test loading one LoRA adapter\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION: Testing LoRA adapter loading\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Test with first model\n",
    "    test_lora_path = FINETUNED_MODELS[0]\n",
    "    print(f\"\\nTesting: {test_lora_path}\")\n",
    "    \n",
    "    # Check files\n",
    "    import os\n",
    "    files = os.listdir(test_lora_path)\n",
    "    print(f\"\\nFiles in adapter directory:\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f}\")\n",
    "    \n",
    "    # Try loading\n",
    "    print(f\"\\nLoading base model: {BASE_MODEL}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cpu\"\n",
    "    )\n",
    "    print(\"  ‚úì Base model loaded\")\n",
    "    \n",
    "    print(f\"\\nLoading LoRA adapter: {test_lora_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, test_lora_path)\n",
    "    print(\"  ‚úì LoRA adapter loaded\")\n",
    "    \n",
    "    print(\"\\nMerging LoRA weights...\")\n",
    "    model = model.merge_and_unload()\n",
    "    print(\"  ‚úì Merge successful!\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del base_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì VERIFICATION PASSED - Ready to proceed!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úó VERIFICATION FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nError: {str(e)}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. BASE_MODEL path is correct\")\n",
    "    print(\"2. LoRA adapters were copied correctly\")\n",
    "    print(\"3. adapter_config.json and adapter_model.* files exist\")\n",
    "    print(\"=\"*80)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3ae60",
   "metadata": {},
   "source": [
    "## Step 3A: Verify Setup (Optional but Recommended)\n",
    "\n",
    "Let's verify everything is configured correctly before starting the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd80097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T07:00:25.549851Z",
     "iopub.status.busy": "2025-11-17T07:00:25.549211Z",
     "iopub.status.idle": "2025-11-17T07:00:25.554682Z",
     "shell.execute_reply": "2025-11-17T07:00:25.554149Z",
     "shell.execute_reply.started": "2025-11-17T07:00:25.549828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Merger initialized\n"
     ]
    }
   ],
   "source": [
    "# Create merger instance\n",
    "merger = LLaMAMerger(\n",
    "    base_model_path=BASE_MODEL,\n",
    "    finetuned_model_paths=FINETUNED_MODELS,\n",
    "    dataset_names=DATASETS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    density=DENSITY,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"‚úì Merger initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e4bdd",
   "metadata": {},
   "source": [
    "## Step 5: Run All Methods and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c8cf08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T07:00:25.555614Z",
     "iopub.status.busy": "2025-11-17T07:00:25.555333Z",
     "iopub.status.idle": "2025-11-17T07:00:45.688233Z",
     "shell.execute_reply": "2025-11-17T07:00:45.687097Z",
     "shell.execute_reply.started": "2025-11-17T07:00:25.555587Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "ERROR:llama_merge:  Failed to load model: Can't find 'adapter_config.json' at './finetuned_model_1'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at './finetuned_model_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/kaggle/working/test-2/llama_merge.py\u001b[0m in \u001b[0;36m_load_finetuned_model\u001b[0;34m(self, ft_model_path, base_model_path)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;31m# Try loading as full model first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 \u001b[0mft_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    548\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1239\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in ./finetuned_model_1. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dots1, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars, '-', '_' or '.'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: './finetuned_model_1'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/323439507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 4. Evaluate and compare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/test-2/llama_merge.py\u001b[0m in \u001b[0;36mmerge_all_methods\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;31m# Step 1: Compute task vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_and_save_task_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;31m# Step 2: Compute Hessians\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/test-2/llama_merge.py\u001b[0m in \u001b[0;36mcompute_and_save_task_vectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;31m# Load fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Loading fine-tuned model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mft_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_finetuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m# Get state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/test-2/llama_merge.py\u001b[0m in \u001b[0;36m_load_finetuned_model\u001b[0;34m(self, ft_model_path, base_model_path)\u001b[0m\n\u001b[1;32m    321\u001b[0m                     \u001b[0mbase_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 )\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                 \u001b[0;31m# Merge LoRA weights into base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_and_unload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 440\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 )\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at './finetuned_model_1'"
     ]
    }
   ],
   "source": [
    "# This will:\n",
    "# 1. Compute task vectors (if not cached)\n",
    "# 2. Compute Hessians (if not cached)\n",
    "# 3. Merge with all three methods\n",
    "# 4. Evaluate and compare\n",
    "\n",
    "results = merger.merge_all_methods()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Perplexity: {metrics['perplexity']:.4f}\")\n",
    "    print(f\"  Time: {metrics['time']:.2f}s\")\n",
    "\n",
    "# Find best method\n",
    "best_method = min(results, key=lambda k: results[k]['perplexity'])\n",
    "print(f\"\\nüèÜ Best method: {best_method}\")\n",
    "print(f\"   Perplexity: {results[best_method]['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab178e",
   "metadata": {},
   "source": [
    "## Step 6: Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a7b50",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-17T07:00:45.688679Z",
     "iopub.status.idle": "2025-11-17T07:00:45.688924Z",
     "shell.execute_reply": "2025-11-17T07:00:45.688823Z",
     "shell.execute_reply.started": "2025-11-17T07:00:45.688806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The models are already saved in OUTPUT_DIR\n",
    "# You can load and test them:\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the best model (usually TIES-SparseGPT)\n",
    "best_model_path = f\"{OUTPUT_DIR}/ties_sparsegpt_merged\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(best_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "print(f\"‚úì Loaded best model from {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5202e",
   "metadata": {},
   "source": [
    "## Step 7: Test the Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d1822",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-17T07:00:45.689948Z",
     "iopub.status.idle": "2025-11-17T07:00:45.690225Z",
     "shell.execute_reply": "2025-11-17T07:00:45.690118Z",
     "shell.execute_reply.started": "2025-11-17T07:00:45.690104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test with a sample prompt\n",
    "prompt = \"I've been feeling anxious lately. What should I do?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc83c2d",
   "metadata": {},
   "source": [
    "## Optional: Run Individual Methods\n",
    "\n",
    "If you want to run just one method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bedf4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-17T07:00:45.691908Z",
     "iopub.status.idle": "2025-11-17T07:00:45.692314Z",
     "shell.execute_reply": "2025-11-17T07:00:45.692163Z",
     "shell.execute_reply.started": "2025-11-17T07:00:45.692140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run only SparseGPT method\n",
    "sparsegpt_model = merger.merge_with_ties(use_sparsegpt=True)\n",
    "sparsegpt_model.save_pretrained(f\"{OUTPUT_DIR}/sparsegpt_only\")\n",
    "\n",
    "print(\"‚úì SparseGPT model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3688e",
   "metadata": {},
   "source": [
    "## Optional: Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43719e7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-17T07:00:45.693361Z",
     "iopub.status.idle": "2025-11-17T07:00:45.693637Z",
     "shell.execute_reply": "2025-11-17T07:00:45.693509Z",
     "shell.execute_reply.started": "2025-11-17T07:00:45.693494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Upload the best model to HuggingFace\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_name = \"your-username/merged-mental-health-counselor\"\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"‚úì Model uploaded to {repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8aee6",
   "metadata": {},
   "source": [
    "## Memory Monitoring (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4875f9a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-17T07:00:45.695054Z",
     "iopub.status.idle": "2025-11-17T07:00:45.695338Z",
     "shell.execute_reply": "2025-11-17T07:00:45.695206Z",
     "shell.execute_reply.started": "2025-11-17T07:00:45.695189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Clear cache if needed\n",
    "    # torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 278867543,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 278870097,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 121027,
     "modelInstanceId": 100933,
     "sourceId": 120002,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
